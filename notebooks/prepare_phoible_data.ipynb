{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Phoible](https://phoible.org/) dataset contains phoneme inventories for thousands of languages and dialects. Many languages/dialicts have multiple Phoible records. Here, I'm mapping the data against pre-prepared IPA phoneme tables, then selecting one sample per table per language so that each language is only represented once in the final dataset (to avoid bias by oversampling).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "phoible_data_url = \"https://raw.githubusercontent.com/phoible/dev/v2.0/data/phoible.csv\"\n",
    "phobile_file_path = \"./downloads/phoible.csv\"\n",
    "\n",
    "VALIDATE_RESULTS = True\n",
    "\n",
    "if not Path(phobile_file_path).exists():\n",
    "    response = requests.get(phoible_data_url, stream=True)\n",
    "\n",
    "    with open(phobile_file_path, \"wb\") as fh:\n",
    "        for data in tqdm(response.iter_content()):\n",
    "            fh.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "phoible_df = pd.read_csv(phobile_file_path, encoding=\"utf-8\", engine=\"python\")\n",
    "\n",
    "ipa_diacritics_df = pd.read_csv(\n",
    "    \"./data/ipa_diacritics.csv\", dtype=str, encoding=\"utf-8\", engine=\"python\"\n",
    ")\n",
    "\n",
    "valid_sufixes = set(ipa_diacritics_df.suffix)\n",
    "\n",
    "phoible_df[\"symbol\"] = phoible_df.Phoneme.apply(lambda x: x[0:-1] if len(x) > 1 else x)\n",
    "phoible_df[\"suffix\"] = phoible_df.Phoneme.apply(\n",
    "    lambda x: x[-1] if (len(x) > 1 and x[-1] in valid_sufixes) else None\n",
    ")\n",
    "\n",
    "lang_by_dialect_df = (\n",
    "    phoible_df[[\"LanguageName\", \"SpecificDialect\", \"Phoneme\", \"symbol\", \"suffix\"]]\n",
    "    .fillna({\"SpecificDialect\": \"none\"})\n",
    "    .groupby([\"LanguageName\", \"SpecificDialect\"])\n",
    ")\n",
    "\n",
    "language_names = (\n",
    "    phoible_df[[\"LanguageName\", \"SpecificDialect\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .to_numpy()[:, 0]\n",
    ")\n",
    "\n",
    "num_lang = len(lang_by_dialect_df)\n",
    "\n",
    "print(\"num_lang\", num_lang)\n",
    "print(phoible_df.shape)\n",
    "\n",
    "phoible_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util\n",
    "\n",
    "\n",
    "def map_long_phonemes(c):\n",
    "    return c + \"ː\" if isinstance(c, str) else c\n",
    "\n",
    "\n",
    "def create_symbol_matrix(phoneme_tbl_df):\n",
    "    phoneme_long_tbl_df = phoneme_tbl_df.map(map_long_phonemes)\n",
    "    symbols = set(phoneme_tbl_df.stack().replace(\"\", np.nan).dropna().unique().tolist())\n",
    "    symbols_long = set([c + \"ː\" for c in symbols])\n",
    "\n",
    "    def map_phonemes(df):\n",
    "        phonemes = set(df.Phoneme.to_list())\n",
    "        # sym_new = df.symbol.to_numpy()\n",
    "        # suffixes = df.suffix.to_numpy()\n",
    "        valid_symbols = phonemes.intersection(symbols)\n",
    "        valid_symbols_long = phonemes.intersection(symbols_long)\n",
    "\n",
    "        # print(\"phoneme_tbl_df\", phoneme_tbl_df.head(4))\n",
    "\n",
    "        # phoneme_tbl_symbols = phoneme_tbl_df.map(lambda x: x[0:-1] if len(x) > 1 else x)\n",
    "        # phoneme_tbl_sufixes = phoneme_tbl_df.map(\n",
    "        #     lambda x: x[-1] if (len(x) > 1 and x[-1] in valid_sufixes) else None\n",
    "        # )\n",
    "\n",
    "        standard = np.where(phoneme_tbl_df.isin(valid_symbols), 1.0, 0.0)\n",
    "        long = np.where(phoneme_long_tbl_df.isin(valid_symbols_long), 1.0, 0.0)\n",
    "        empty = np.zeros(standard.shape)\n",
    "\n",
    "        # print(\"df\", df[[\"symbol\", \"suffix\"]].head(4))\n",
    "        # print(\"phoneme_tbl_symbols\", phoneme_tbl_symbols.iloc[:4, :4])\n",
    "        # print(\"phoneme_tbl_sufixes\", phoneme_tbl_sufixes.iloc[:4, :4])\n",
    "        # print(\"standard\", standard[:4, :4])\n",
    "\n",
    "        result = np.stack([standard, long, empty], axis=2)\n",
    "\n",
    "        if VALIDATE_RESULTS:\n",
    "            for row, _ in enumerate(standard):\n",
    "                for col, _ in enumerate(standard.T):\n",
    "                    assert standard[row][col] + long[row][col] == result[row][col].sum()\n",
    "\n",
    "            assert standard.sum() == len(valid_symbols)\n",
    "            assert long.sum() == len(valid_symbols_long)\n",
    "            assert result.sum() == standard.sum() + long.sum() + empty.sum()\n",
    "\n",
    "        return result\n",
    "\n",
    "    return map_phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pulomic consonants table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_pl_tbl_df = pd.read_csv(\"./data/consonants_plumonic.csv\", dtype=str, index_col=[0], keep_default_na=False, na_values=[\"-1\"])\n",
    "cons_pl_tbl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_pl_npy = np.stack(lang_by_dialect_df.apply(create_symbol_matrix(cons_pl_tbl_df), include_groups=False).to_numpy())\n",
    "assert cons_pl_npy.shape == (num_lang, *cons_pl_tbl_df.shape, 3)\n",
    "cons_pl_npy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate non-pulomic consonants table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_npl_tbl_df = pd.read_csv(\n",
    "    \"./data/consonants_non_plumonic.csv\",\n",
    "    dtype=str,\n",
    "    index_col=[0],\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"-1\"],\n",
    ")\n",
    "cons_npl_tbl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_npl_npy = np.stack(\n",
    "    lang_by_dialect_df.apply(\n",
    "        create_symbol_matrix(cons_npl_tbl_df), include_groups=False\n",
    "    ).to_numpy()\n",
    ")\n",
    "assert cons_npl_npy.shape == (num_lang, *cons_npl_tbl_df.shape, 3)\n",
    "cons_npl_npy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_coart_tbl_df = pd.read_csv(\n",
    "    \"./data/consonants_coarticulated.csv\",\n",
    "    dtype=str,\n",
    "    index_col=[0],\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"-1\"],\n",
    ")\n",
    "cons_coart_tbl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_coart_npy = np.stack(\n",
    "    lang_by_dialect_df.apply(\n",
    "        create_symbol_matrix(cons_coart_tbl_df), include_groups=False\n",
    "    ).to_numpy()\n",
    ")\n",
    "assert cons_coart_npy.shape == (num_lang, *cons_coart_tbl_df.shape, 3)\n",
    "cons_coart_npy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate vowel table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels_tbl_df = pd.read_csv(\n",
    "    \"./data/vowels.csv\",\n",
    "    dtype=str,\n",
    "    index_col=[0],\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"-1\"],\n",
    ")\n",
    "vowels_tbl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels_npy = np.stack(\n",
    "    lang_by_dialect_df.apply(\n",
    "        create_symbol_matrix(vowels_tbl_df), include_groups=False\n",
    "    ).to_numpy()\n",
    ")\n",
    "assert vowels_npy.shape == (num_lang, *vowels_tbl_df.shape, 3)\n",
    "vowels_npy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    cons_pl_npy.shape[0]\n",
    "    == cons_npl_npy.shape[0]\n",
    "    == vowels_npy.shape[0]\n",
    "    == cons_coart_npy.shape[0]\n",
    ")\n",
    "\n",
    "(cons_pl_npy.shape, cons_npl_npy.shape, vowels_npy.shape, cons_coart_npy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For languages that have multiple samples in the Phoible dataset, we pick just one sample. Based on previous analysis, the best option seems to be to just pick the one whith the most phonemes per language, as this generally has more information (for example long and short versions of the phonemes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = np.unique(language_names)\n",
    "print(language_names.shape, unique_names.shape)\n",
    "\n",
    "cons_pl_selected = np.array(\n",
    "    [np.max(cons_pl_npy[language_names == name], axis=0) for name in unique_names]\n",
    ")\n",
    "cons_npl_selected = np.array(\n",
    "    [np.max(cons_npl_npy[language_names == name], axis=0) for name in unique_names]\n",
    ")\n",
    "cons_coart_selected = np.array(\n",
    "    [np.max(cons_coart_npy[language_names == name], axis=0) for name in unique_names]\n",
    ")\n",
    "vowels_selected = np.array(\n",
    "    [np.max(vowels_npy[language_names == name], axis=0) for name in unique_names]\n",
    ")\n",
    "\n",
    "# sense check\n",
    "for name in unique_names:\n",
    "    assert cons_pl_npy[language_names == name].shape[1:] == cons_pl_selected.shape[1:]\n",
    "    assert cons_npl_npy[language_names == name].shape[1:] == cons_npl_selected.shape[1:]\n",
    "    assert vowels_npy[language_names == name].shape[1:] == vowels_selected.shape[1:]\n",
    "    assert cons_coart_npy[language_names == name].shape[1:] == cons_coart_selected.shape[1:]\n",
    "\n",
    "cons_pl_selected.shape, cons_npl_selected.shape, vowels_selected.shape, cons_coart_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./data/consonants_plumonic.npy\", cons_pl_selected)\n",
    "np.save(\"./data/consonants_non_plumonic.npy\", cons_npl_selected)\n",
    "np.save(\"./data/consonants_coarticulated.npy\", cons_coart_selected)\n",
    "np.save(\"./data/vowels.npy\", vowels_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all the data into a single 4d array. Each language is represented by a 3d matrix, with columns of the 3 phoneme types stacked vertically in 3 groups and a padding row between each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cols = max(cons_pl_npy.shape[2], cons_npl_npy.shape[2], vowels_npy.shape[2])\n",
    "max_value = max(np.max(cons_pl_npy), np.max(cons_npl_npy), np.max(vowels_npy))\n",
    "\n",
    "\n",
    "def get_padding(arr, max_cols):\n",
    "    return ((0, 0), (0, 0), (0, max_cols - arr.shape[2]), (0, 0))\n",
    "\n",
    "\n",
    "language_phonemes_npy = np.hstack(\n",
    "    [\n",
    "        np.pad(\n",
    "            cons_pl_npy,\n",
    "            get_padding(cons_pl_npy, max_cols),\n",
    "            mode=\"constant\",\n",
    "        ),\n",
    "        np.full((cons_pl_npy.shape[0], 1, max_cols, 3), 0.1),\n",
    "        np.pad(\n",
    "            cons_npl_npy,\n",
    "            get_padding(cons_npl_npy, max_cols),\n",
    "            mode=\"constant\",\n",
    "        ),\n",
    "        np.full((cons_pl_npy.shape[0], 1, max_cols, 3), 0.1),\n",
    "        np.pad(vowels_npy, get_padding(vowels_npy, max_cols), mode=\"constant\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "language_phonemes_npy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r, c = 2, 4\n",
    "\n",
    "gen = np.random.default_rng()\n",
    "sample_indices = gen.choice(unique_names.size, r * c, replace=False)\n",
    "samples = language_phonemes_npy[sample_indices]\n",
    "sample_names = unique_names[sample_indices]\n",
    "\n",
    "fig, axs = plt.subplots(r, c, figsize=(22, 12), linewidth=10)\n",
    "fig.tight_layout()\n",
    "\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        axs[i, j].set_title(sample_names[cnt][0:20], fontsize=26)\n",
    "        axs[i, j].imshow(samples[cnt])\n",
    "        axs[i, j].tick_params(\n",
    "            left=False, right=False, labelleft=False, labelbottom=False, bottom=False\n",
    "        )\n",
    "        cnt += 1\n",
    "\n",
    "plt.show()\n",
    "sample_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./data/language_phonemes.npy\", language_phonemes_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does an average language look like?\n",
    "mean_lang = np.mean(language_phonemes_npy, axis=0)\n",
    "\n",
    "# not required, but just checking that imshow \n",
    "# plays nice with an alpha channel\n",
    "mean_lang = np.dstack([mean_lang, np.ones((*mean_lang.shape[0:2], 1))])\n",
    "\n",
    "fig = plt.imshow(\n",
    "    mean_lang,\n",
    "    cmap=plt.get_cmap(\"copper_r\"),\n",
    "    vmin=np.min(mean_lang),\n",
    "    vmax=np.max(mean_lang),\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Average Language\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating unhandled phonemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handled_symbols = (\n",
    "    set(cons_pl_tbl_df.stack().replace(\"\", np.nan).dropna().unique())\n",
    "    | set(cons_npl_tbl_df.stack().replace(\"\", np.nan).dropna().unique())\n",
    "    | set(cons_coart_tbl_df.stack().replace(\"\", np.nan).dropna().unique())\n",
    "    | set(vowels_tbl_df.stack().replace(\"\", np.nan).dropna().unique())\n",
    ")\n",
    "handled_symbols_long = set([c + \"ː\" for c in handled_symbols])\n",
    "all_handled_symbols = handled_symbols | handled_symbols_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to replace the long implementation above with something that can represent all of these:\n",
    "print(ipa_diacritics_df.suffix.shape)\n",
    "# ipa_diacritics_df.suffix.to_numpy().reshape(6, -1).T\n",
    "\n",
    "diacritics_npy = ipa_diacritics_df.suffix.to_numpy()\n",
    "\n",
    "\n",
    "def create_symbol_matrix2(phoneme_tbl_df):\n",
    "    symbols = set(\n",
    "        phoneme_tbl_df.stack().replace(\"\", np.nan).dropna().unique().tolist()\n",
    "    )\n",
    "\n",
    "    def map_phonemes(df):\n",
    "        phonemes = set(df.Phoneme.to_list())\n",
    "        valid_symbols = phonemes.intersection(symbols)\n",
    "\n",
    "        standard = np.where(phoneme_tbl_df.isin(valid_symbols), 1.0, 0.0)\n",
    "        empty = np.zeros(standard.shape)\n",
    "\n",
    "        result = np.stack([standard, empty], axis=2)\n",
    "\n",
    "        return result\n",
    "\n",
    "    return map_phonemes\n",
    "\n",
    "\n",
    "tmp_npy = np.stack(\n",
    "    lang_by_dialect_df.apply(\n",
    "        create_symbol_matrix2(cons_pl_tbl_df), include_groups=False\n",
    "    ).to_numpy()\n",
    ")\n",
    "tmp_npy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sufixes = set(ipa_diacritics_df.suffix)\n",
    "\n",
    "all_phonemes = phoible_df.Phoneme.drop_duplicates().to_frame()\n",
    "all_phonemes[\"symbol\"] = all_phonemes.Phoneme.apply(\n",
    "    lambda x: x[0:-1] if len(x) > 1 else x\n",
    ")\n",
    "all_phonemes[\"suffix\"] = all_phonemes.Phoneme.apply(\n",
    "    lambda x: x[-1] if (len(x) > 1 and x[-1] in valid_sufixes) else None\n",
    ")\n",
    "all_phonemes = all_phonemes[[\"symbol\", \"suffix\"]].drop_duplicates()\n",
    "\n",
    "# all_phonemes[[\"symbol\", \"suffix\"]]\n",
    "all_phonemes.suffix.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhandled_symbols = phoible_df[~phoible_df.Phoneme.isin(all_handled_symbols)][\n",
    "    [\"LanguageName\", \"SpecificDialect\", \"Phoneme\", \"symbol\", \"suffix\"]\n",
    "]\n",
    "\n",
    "unhandled_symbols_by_dialect = (\n",
    "    unhandled_symbols.fillna({\"SpecificDialect\": \"none\"})\n",
    "    .drop_duplicates()\n",
    "    .groupby([\"LanguageName\", \"SpecificDialect\"])\n",
    ")\n",
    "\n",
    "unhandled_counts = (\n",
    "    unhandled_symbols_by_dialect.count()\n",
    "    .reset_index()\n",
    "    .groupby(\"LanguageName\")\n",
    "    .mean(\"Phoneme\")\n",
    "    .reset_index()\n",
    "    .sort_values(\"Phoneme\", ascending=False)\n",
    ")\n",
    "\n",
    "unhandled_counts.plot.hist(bins=75)\n",
    "\n",
    "print(unhandled_symbols)\n",
    "\n",
    "unhandled_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
